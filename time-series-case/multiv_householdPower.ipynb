{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7638b7ac",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "Kasus ini akan melakukan time series analysis dari dataset bernama Daily Climate Series.\n",
    "Link Dataset: https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data\n",
    "\n",
    "Kasusnya adalah ingin memprediksi cuaca.\n",
    "\n",
    "Notes: \n",
    "Belum ada info diawalain itu memprediksi di satu langkah kedepan (1 hari) atau beberapa langkah kedepan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e57a9",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ece069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dicoding/Dicoding/github/mlops/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb5475",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbef509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8j/v2ynd5256ylfs21m0cbfj50h0000gp/T/ipykernel_27337/2880810592.py:1: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(\"https://github.com/ridhaginanjar/mlops-explore/releases/download/household-time-series/household_power_consumption.csv\", sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:24:00</th>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.84</td>\n",
       "      <td>18.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:25:00</th>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.63</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:26:00</th>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.29</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:27:00</th>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.74</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-16 17:28:00</th>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.68</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Global_active_power  Global_reactive_power  Voltage  \\\n",
       "datetime                                                                   \n",
       "2006-12-16 17:24:00                4.216                  0.418   234.84   \n",
       "2006-12-16 17:25:00                5.360                  0.436   233.63   \n",
       "2006-12-16 17:26:00                5.374                  0.498   233.29   \n",
       "2006-12-16 17:27:00                5.388                  0.502   233.74   \n",
       "2006-12-16 17:28:00                3.666                  0.528   235.68   \n",
       "\n",
       "                     Global_intensity  Sub_metering_1  Sub_metering_2  \\\n",
       "datetime                                                                \n",
       "2006-12-16 17:24:00              18.4             0.0             1.0   \n",
       "2006-12-16 17:25:00              23.0             0.0             1.0   \n",
       "2006-12-16 17:26:00              23.0             0.0             2.0   \n",
       "2006-12-16 17:27:00              23.0             0.0             1.0   \n",
       "2006-12-16 17:28:00              15.8             0.0             1.0   \n",
       "\n",
       "                     Sub_metering_3  \n",
       "datetime                             \n",
       "2006-12-16 17:24:00            17.0  \n",
       "2006-12-16 17:25:00            16.0  \n",
       "2006-12-16 17:26:00            17.0  \n",
       "2006-12-16 17:27:00            17.0  \n",
       "2006-12-16 17:28:00            17.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://github.com/ridhaginanjar/mlops-explore/releases/download/household-time-series/household_power_consumption.csv\", sep=',', infer_datetime_format=True, index_col='datetime', header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f19f11",
   "metadata": {},
   "source": [
    "# Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151fce2",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1f9d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize_series(data, min, max):\n",
    "    data = data - min\n",
    "    data = data / max\n",
    "    return data\n",
    "\n",
    "data = df.values\n",
    "data = normalize_series(data, data.min(axis=0), data.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c8b12",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dcf817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43377912, 0.47826087, 0.04036551, ..., 0.        , 0.01282051,\n",
       "        0.85      ],\n",
       "       [0.55716135, 0.49885584, 0.0355582 , ..., 0.        , 0.01282051,\n",
       "        0.8       ],\n",
       "       [0.55867127, 0.56979405, 0.03420739, ..., 0.        , 0.02564103,\n",
       "        0.85      ],\n",
       "       ...,\n",
       "       [0.03710095, 0.        , 0.05983313, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03559103, 0.        , 0.06515693, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03774806, 0.        , 0.06730234, ..., 0.        , 0.01282051,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0272a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = len(df.columns)\n",
    "SPLIT_TIME = int(len(data) * 0.5)\n",
    "x_train = data[:SPLIT_TIME]\n",
    "x_valid = data[SPLIT_TIME:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series) # Mengubah series menjadi dataset tensorflow\n",
    "    ds = ds.window(n_past + n_future, shift=shift, drop_remainder=True) # Melakukan windowing (dalam kasus ini 60 +1)\n",
    "    ds = ds.flat_map(lambda x:x.batch(n_past + n_future)) # Melakukan flattening -> e.g: [[1],[2],[3],[4],[5]] menjadi [1,2,3] [2,3,4] [3,4,5]\n",
    "    ds = ds.map(lambda x: (x[:n_past], x[n_future:])) # memecah x dan y -> x = [[2,3]] y =[[4]]\n",
    "    return ds.batch(batch_size).prefetch(1) # mengelompokkan per batch -> e.g: batch 1 x = [[2,3]] y =[[4]], batch 2 = xxxx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc0413c",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f52e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_PAST = 24\n",
    "N_FUTURE = 24\n",
    "SHIFT = 1\n",
    "\n",
    "train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE, n_past=N_PAST, n_future=N_FUTURE, shift=SHIFT)\n",
    "valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE, n_past=N_PAST, n_future=N_FUTURE, shift=SHIFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a4a87",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98fe1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dicoding/Dicoding/github/mlops/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_shape=(N_PAST, N_FEATURES)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(N_FEATURES)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ce1c6",
   "metadata": {},
   "source": [
    "## Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7bddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallBacks(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('mae') < 0.555 and logs.get('val_mae') < 0.555):\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallBacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff0a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss='mae', optimizer=optimizer, metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e4e3f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7516223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1318/Unknown \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0744 - mae: 0.0744"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:54:34.816728: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Users/dicoding/Dicoding/github/mlops/.venv/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0742 - mae: 0.0742 - val_loss: 0.0602 - val_mae: 0.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:54:36.445868: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, validation_data=(valid_set), epochs=100, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d76b9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb55c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1349/1349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.2658706 ,  0.12326753,  0.04302836,  0.28176   , -0.011466  ,\n",
       "       -0.00538698,  0.88532746], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = model.predict(train_set)\n",
    "train_pred[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
